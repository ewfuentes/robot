{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a0d6a-5cb9-4313-9a23-86aeb9bd025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a3c4f-1f36-45f4-bdaf-ea49cf50f0f8",
   "metadata": {},
   "source": [
    "\n",
    "Given the following distribution:\n",
    "\n",
    "$P(z) = Exp(\\lambda=1) = \\lambda \\exp(-\\lambda z)$\n",
    "\n",
    "$P(x|z) = \\textrm{Normal}(\\mu=z, \\sigma^2=1)$\n",
    "\n",
    "We want to model $P(z|x)$, which is commonly known as the posterior distribution.\n",
    "\n",
    "Typically, this quantity could be computed using Bayes rule.\n",
    "\n",
    "$P(z|x) = \\frac{P(x|z)P(z)}{P(x)} = \\frac{P(x|z)P(z)}{\\int P(x|z)P(z) dz}$\n",
    "\n",
    "We note that the integral in the denominator is intractable, so instead, we choose a surrogate distribution $q_{\\theta}(z)$ and we try to get it as close as possible to $P(z|x)$. We attempt to find the $\\theta$ such that the the probability of the observed $x$ is maximized. That is:\n",
    "\n",
    "$$\\arg\\min_{q_{\\theta}} -E_{z \\sim q_{\\theta}(z)}[\\log P(x)]$$\n",
    "\n",
    "We note that:\n",
    "\\begin{align}\n",
    "\\log P(x)  = E_{z \\sim q_{\\theta}(z)}[\\log P(x)] &= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(P(x) \\frac{q_{\\theta}(z)}{q_{\\theta}(z)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{P(z|x)} \\frac{q_{\\theta}(z)}{q_{\\theta}(z)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{q_{\\theta}(z)} \\frac{q_{\\theta}(z)}{P(z|x)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{q_{\\theta}(z)}\\right) + \\log \\left( \\frac{q_{\\theta}(z)}{P(z|x)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{q_{\\theta}(z)}\\right)\\right] + E_{z \\sim q_{\\theta}(z)}\\left[ \\log \\left( \\frac{q_{\\theta}(z)}{P(z|x)}\\right)\\right] \\\\\n",
    "&= \\mathcal{L_{\\theta}} + D_{KL}(q_{\\theta}(z) || P(z|x)) \\\\\n",
    "\\log P(x) &\\geq \\mathcal{L_{\\theta}}\n",
    "\\end{align}\n",
    "\n",
    "Since, the KL divergence is non-negative, $\\mathcal{L_{\\theta}}$ is known as the evidence lower bound, or the ELBO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc766c-0997-47c2-bfd7-84d86cd4ec59",
   "metadata": {},
   "source": [
    "Assume that we have observed a dataset $\\mathcal{D} = \\{x_i\\}_{i=1}^{N}$. Then, $P(\\mathcal{D}|z) = \\prod_{i=1}^{N} P(x_i|z)$.\n",
    "\n",
    "The ELBO is then:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\theta} &= E_{z\\sim q_{\\theta}(z)}[\\log(P(\\mathcal{D}, z)) - \\log(q_{\\theta}(z))] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}[\\log(P(\\mathcal{D}|z)P(z)) - \\log q_{\\theta}(z)] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}[\\log P(\\mathcal{D}|z) + \\log P(z) - \\log q_{\\theta}(z)] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}\\left[\\sum_{i=1}^{N}\\log P(x_i|z) + \\log P(z) - \\log q_{\\theta}(z)\\right] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}\\left[-\\frac{1}{2}\\sum_{i=1}^{N} (x_i - z)^2 - \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}\\left[-\\frac{1}{2}\\sum_{i=1}^{N} (x_i^2 -2 x_i z + z^2) - \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right]\n",
    "\\end{align}\n",
    "\n",
    "This equation is true for all $q_{\\theta}(z)$. Now we assume that $q_{\\theta}(z) = Exp(\\theta)$.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\theta} &= E_{z\\sim q_{\\theta}(z)}\\left[-\\frac{1}{2}\\sum_{i=1}^{N} (x_i^2 -2 x_i z + z^2) - \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right] \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N} E_{z\\sim q_{\\theta}(z)}\\left[x_i^2 -2 x_i z + z^2\\right] + E_{z\\sim q_{\\theta}(z)}\\left[- \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right] \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N} \\left(x_i^2 - \\frac{2}{\\theta}x_i + \\frac{2}{\\theta^2}\\right) - \\frac{N}{2} \\log(2\\pi) + \\log\\lambda - \\frac{\\lambda}{\\theta} - \\log\\theta + 1 \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N} x_i^2 + \\frac{1}{\\theta}\\sum_{i=1}^N x_i - \\frac{N}{\\theta^2} - \\frac{N}{2} \\log(2\\pi) + \\log\\lambda - \\frac{\\lambda}{\\theta} - \\log\\theta + 1 \\\\\n",
    "&= \\frac{1}{\\theta} \\left(\\sum_{i=1}^{N} (x_i) - \\lambda \\right) - \\frac{N}{\\theta^2} - \\log \\theta + C\n",
    "\\end{align}\n",
    "\n",
    "To compute the optimal $\\theta^*$, we need to compute the $\\theta$ for which $\\nabla_{\\theta} \\mathcal{L}_{\\theta} = 0$. Let $B = \\sum_{i=1}^{N} (x_i) - \\lambda$, then:\n",
    "\n",
    "\\begin{align}\n",
    "0 &= \\nabla_{\\theta} \\mathcal{L}_{\\theta} \\\\\n",
    "&= \\nabla_{\\theta} \\left(\\frac{1}{\\theta} B - \\frac{N}{\\theta^2} - \\log \\theta + C\\right) \\\\\n",
    "&= \\frac{-1}{\\theta^2} B + \\frac {2N}{\\theta^3} - \\frac{1}{\\theta} \\\\\n",
    "&= - \\theta^2 -\\theta B + 2N \n",
    "\\end{align}\n",
    "\n",
    "This is a quadratic equation in $\\theta$ with solutions:\n",
    "$$\\theta^* = \\frac{-B}{2} \\pm \\frac{1}{2}\\sqrt{B^2 + 8 N}$$\n",
    "\n",
    "Note that $\\theta$ is constrained to be positive, so one of the solutions may not be feasible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb228e-d257-44cf-8355-274bb0f295fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
