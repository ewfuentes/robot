{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a0d6a-5cb9-4313-9a23-86aeb9bd025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "import common.torch as torch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import math\n",
    "mpl.style.use('ggplot')\n",
    "\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a3c4f-1f36-45f4-bdaf-ea49cf50f0f8",
   "metadata": {},
   "source": [
    "\n",
    "Given the following distribution:\n",
    "\n",
    "$P(z) = Exp(\\lambda=1) = \\lambda \\exp(-\\lambda z)$\n",
    "\n",
    "$P(x|z) = \\textrm{Normal}(\\mu=z, \\sigma^2=1)$\n",
    "\n",
    "We want to model $P(z|x)$, which is commonly known as the posterior distribution.\n",
    "\n",
    "Typically, this quantity could be computed using Bayes rule.\n",
    "\n",
    "$P(z|x) = \\frac{P(x|z)P(z)}{P(x)} = \\frac{P(x|z)P(z)}{\\int P(x|z)P(z) dz}$\n",
    "\n",
    "We note that the integral in the denominator is intractable, so instead, we choose a surrogate distribution $q_{\\theta}(z)$ and we try to get it as close as possible to $P(z|x)$. We attempt to find the $\\theta$ such that the the probability of the observed $x$ is maximized. That is:\n",
    "\n",
    "$$\\arg\\min_{q_{\\theta}} -E_{z \\sim q_{\\theta}(z)}[\\log P(x)]$$\n",
    "\n",
    "We note that:\n",
    "\\begin{align}\n",
    "\\log P(x)  = E_{z \\sim q_{\\theta}(z)}[\\log P(x)] &= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(P(x) \\frac{q_{\\theta}(z)}{q_{\\theta}(z)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{P(z|x)} \\frac{q_{\\theta}(z)}{q_{\\theta}(z)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{q_{\\theta}(z)} \\frac{q_{\\theta}(z)}{P(z|x)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{q_{\\theta}(z)}\\right) + \\log \\left( \\frac{q_{\\theta}(z)}{P(z|x)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{q_{\\theta}(z)}\\right)\\right] + E_{z \\sim q_{\\theta}(z)}\\left[ \\log \\left( \\frac{q_{\\theta}(z)}{P(z|x)}\\right)\\right] \\\\\n",
    "&= \\mathcal{L_{\\theta}} + D_{KL}(q_{\\theta}(z) || P(z|x)) \\\\\n",
    "\\log P(x) &\\geq \\mathcal{L_{\\theta}}\n",
    "\\end{align}\n",
    "\n",
    "Since, the KL divergence is non-negative, $\\mathcal{L_{\\theta}}$ is known as the evidence lower bound, or the ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc766c-0997-47c2-bfd7-84d86cd4ec59",
   "metadata": {},
   "source": [
    "Assume that we have observed a dataset $\\mathcal{D} = \\{x_i\\}_{i=1}^{N}$. Then, $P(\\mathcal{D}|z) = \\prod_{i=1}^{N} P(x_i|z)$.\n",
    "\n",
    "The ELBO is then:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\theta} &= E_{z\\sim q_{\\theta}(z)}[\\log(P(\\mathcal{D}, z)) - \\log(q_{\\theta}(z))] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}[\\log(P(\\mathcal{D}|z)P(z)) - \\log q_{\\theta}(z)] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}[\\log P(\\mathcal{D}|z) + \\log P(z) - \\log q_{\\theta}(z)] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}\\left[\\sum_{i=1}^{N}\\log P(x_i|z) + \\log P(z) - \\log q_{\\theta}(z)\\right] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}\\left[-\\frac{1}{2}\\sum_{i=1}^{N} (x_i - z)^2 - \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}\\left[-\\frac{1}{2}\\sum_{i=1}^{N} (x_i^2 -2 x_i z + z^2) - \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right]\n",
    "\\end{align}\n",
    "\n",
    "This equation is true for all $q_{\\theta}(z)$. Now we assume that $q_{\\theta}(z) = Exp(\\theta)$.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\theta} &= E_{z\\sim q_{\\theta}(z)}\\left[-\\frac{1}{2}\\sum_{i=1}^{N} (x_i^2 -2 x_i z + z^2) - \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right] \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N} E_{z\\sim q_{\\theta}(z)}\\left[x_i^2 -2 x_i z + z^2\\right] + E_{z\\sim q_{\\theta}(z)}\\left[- \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right] \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N} \\left(x_i^2 - \\frac{2}{\\theta}x_i + \\frac{2}{\\theta^2}\\right) - \\frac{N}{2} \\log(2\\pi) + \\log\\lambda - \\frac{\\lambda}{\\theta} - \\log\\theta + 1 \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N} x_i^2 + \\frac{1}{\\theta}\\sum_{i=1}^N x_i - \\frac{N}{\\theta^2} - \\frac{N}{2} \\log(2\\pi) + \\log\\lambda - \\frac{\\lambda}{\\theta} - \\log\\theta + 1 \\\\\n",
    "&= \\frac{1}{\\theta} \\left(\\sum_{i=1}^{N} (x_i) - \\lambda \\right) - \\frac{N}{\\theta^2} - \\log \\theta + C\n",
    "\\end{align}\n",
    "\n",
    "To compute the optimal $\\theta^*$, we need to compute the $\\theta$ for which $\\nabla_{\\theta} \\mathcal{L}_{\\theta} = 0$. Let $B = \\sum_{i=1}^{N} (x_i) - \\lambda$, then:\n",
    "\n",
    "\\begin{align}\n",
    "0 &= \\nabla_{\\theta} \\mathcal{L}_{\\theta} \\\\\n",
    "&= \\nabla_{\\theta} \\left(\\frac{1}{\\theta} B - \\frac{N}{\\theta^2} - \\log \\theta + C\\right) \\\\\n",
    "&= \\frac{-1}{\\theta^2} B + \\frac {2N}{\\theta^3} - \\frac{1}{\\theta} \\\\\n",
    "&= - \\theta^2 -\\theta B + 2N \n",
    "\\end{align}\n",
    "\n",
    "This is a quadratic equation in $\\theta$ with solutions:\n",
    "$$\\theta^* = \\frac{-B}{2} \\pm \\frac{1}{2}\\sqrt{B^2 + 8 N}$$\n",
    "\n",
    "Note that $\\theta$ is constrained to be positive, so one of the solutions may not be feasible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb228e-d257-44cf-8355-274bb0f295fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_lambda = 1.0\n",
    "prior_dist = torch.distributions.Exponential(prior_lambda)\n",
    "torch.manual_seed(0)\n",
    "sampled_z = prior_dist.sample()\n",
    "\n",
    "normal_std = 1.0\n",
    "observation_dist = torch.distributions.Normal(sampled_z, normal_std)\n",
    "sampled_x = observation_dist.sample((5,))\n",
    "# sampled_x = torch.tensor(2.3)\n",
    "\n",
    "print(f'z: {sampled_z} x: {sampled_x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a7b92-b3e1-4e02-b01c-8f41a33cb3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zs = torch.linspace(0, 15, 2000, dtype=torch.float64)\n",
    "joint_evals = []\n",
    "for z in Zs:\n",
    "    obs_dist = torch.distributions.Normal(z, normal_std)\n",
    "    log_obs_prob = torch.sum(obs_dist.log_prob(sampled_x))\n",
    "    joint_evals.append(prior_dist.log_prob(z) + log_obs_prob)\n",
    "joint_evals = torch.exp(torch.stack(joint_evals))\n",
    "approx_posterior = joint_evals / torch.sum(joint_evals * Zs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058305f5-8761-4ecd-be76-83bc97a39456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming an exponential posterior, find the optimal theta\n",
    "B = torch.sum(sampled_x) - prior_lambda\n",
    "theta_star = 0.5 * (-B + torch.sqrt(B**2 + 8 * sampled_x.numel()))\n",
    "display(Math(rf'$\\theta^*={theta_star:3f}$'))\n",
    "variational_posterior = torch.distributions.Exponential(theta_star)\n",
    "variational_evals = torch.exp(variational_posterior.log_prob(Zs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428021f-b6ea-4261-870b-e439726952af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Zs, joint_evals, label='joint')\n",
    "plt.plot(Zs, approx_posterior, label='numerical posterior')\n",
    "plt.plot(Zs, variational_evals, label='variational posterior')\n",
    "plt.xlabel('Z')\n",
    "plt.ylabel('$p(Z|X_{obs})$')\n",
    "plt.xlim(0, 5)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8201f-23fa-4087-9966-9dafb35a7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KL(Zs, numerical_posterior, variational_posterior):\n",
    "    delta = Zs[1]\n",
    "    variational_vals = variational_posterior.log_prob(Zs)\n",
    "    exp_variational_vals = torch.exp(variational_vals)\n",
    "    numerical_vals = torch.log(numerical_posterior)\n",
    "    return delta * torch.sum(exp_variational_vals * (variational_vals - numerical_vals))\n",
    "    \n",
    "thetas = torch.linspace(0.5, 2, 500, dtype=torch.float64)\n",
    "kl_divergences = []\n",
    "for theta in thetas:\n",
    "    kl_divergences.append(compute_KL(Zs, approx_posterior, torch.distributions.Exponential(theta)))\n",
    "    # break\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thetas, kl_divergences)\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.xlabel(r'$\\theta$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c2e97-3812-4153-b271-09c765237384",
   "metadata": {},
   "source": [
    "Note that the plot doesn't show a minimum at the expected location because the KL divergence is only computed for a subset of the range $[0, \\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccfdb2-6557-40f4-b191-56f0be5d6990",
   "metadata": {},
   "source": [
    "Now we try to optimize the surrogate using pytorch.\n",
    "\n",
    "We want to optimize the ELBO:\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\theta} &= E_{z\\sim q_{\\theta}(z)} \\left[\\log \\frac{p(\\mathcal{D},z)}{q_{\\theta}(z)} \\right]\n",
    "\\end{align}\n",
    "\n",
    "Which means that we need to compute $\\nabla_{\\theta}\\mathcal{L}_{\\theta}$. This is difficult because the expectation is with respect to the distribution which is a function of $\\theta$. We then apply the reparameterization trick. Let $q_{\\theta}(z) = g_{\\theta}(x, t)$, where $t\\sim\\epsilon$ is a fixed noise distribution. Then $E_{z\\sim q_{\\theta}}[f(z)] = E_{t \\sim \\epsilon}[f(g_{\\theta}(x, t))]$. This expectation can be estimated by drawing samples from $\\epsilon$ and computing the sample mean of $f(g_{\\theta}(x, t))$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c27ebf-b5d0-4cfa-9207-f2a5a434dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try to train find the parameter through optimization\n",
    "\n",
    "class VariationalModel(torch.nn.Module):\n",
    "    N = 128\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VariationalModel, self).__init__()\n",
    "\n",
    "        self.param = torch.nn.Parameter(data=torch.tensor([1.0]))\n",
    "        self.sampling_dist = torch.distributions.Exponential(1.0)\n",
    "        self.prior = torch.distributions.Exponential(1.0)\n",
    "\n",
    "    def elbo(self, data):\n",
    "        \n",
    "        sampled_z = self.sampling_dist.sample((self.N,))\n",
    "        # Reparameterization trick for exponential distributions\n",
    "        reparametrized_z = sampled_z / self.param\n",
    "\n",
    "        # compute the probability of the data\n",
    "        # p(D, z) = p(z) * prod(p(x_i, z) for x_i in data)\n",
    "        elbo = 0.0\n",
    "        for z in reparametrized_z:\n",
    "            log_p_z = self.prior.log_prob(z)\n",
    "            obs = torch.distributions.Normal(z, 1.0)\n",
    "            log_data_given_z = torch.sum(obs.log_prob(data))\n",
    "\n",
    "            log_model = torch.distributions.Exponential(self.param).log_prob(z)\n",
    "            elbo += log_p_z + log_data_given_z - log_model\n",
    "        return elbo / self.N\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe509060-51d3-46ec-8ee9-39346ed7e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VariationalModel()\n",
    "model.train()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for i in range(1000):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    loss = -model.elbo(sampled_x)\n",
    "    loss.backward()\n",
    "    if i % 100 == 0:\n",
    "        print(i, loss.detach(), model.param.data)\n",
    "    opt.step()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cd66d-73ed-4d21-ba00-be928ffaf874",
   "metadata": {},
   "source": [
    "This is really close to the optimal value we found above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0920c70-4294-42de-b9df-d9ff491d33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try to implement a VAE on MNIST\n",
    "\n",
    "class VariationalAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        # Define the encoder layers\n",
    "        self._encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 8, 3),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(8, 8, 3),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "            torch.nn.Conv2d(8, 16, 3),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(16, 16, 3),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "            torch.nn.Conv2d(16, 32, 2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(32, 32, 2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        self._mean_model = torch.nn.Linear(32, 32)\n",
    "        self._log_sigma_model = torch.nn.Linear(32, 32)\n",
    "\n",
    "        # Define the decoder layers\n",
    "        self._decoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 32, 1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            torch.nn.Conv2d(32, 16, 3, padding=2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(16, 16, 3, padding=2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            torch.nn.Conv2d(16, 8, 3, padding=2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(8, 8, 3, padding=1),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "            torch.nn.Conv2d(8, 1, 3, padding=1),          \n",
    "        )\n",
    "\n",
    "        self._prior_mean = torch.zeros(32)\n",
    "        self._scale_tril = torch.eye(32)\n",
    "        self._prior_dist = torch.distributions.MultivariateNormal(self._prior_mean, scale_tril=self._scale_tril)\n",
    "\n",
    "    def encoder(self, x: torch.Tensor):\n",
    "        '''\n",
    "        Implements p_{\\theta}(z | x)\n",
    "        '''\n",
    "        feature = self._encoder(x)\n",
    "        mean = self._mean_model(feature.squeeze())\n",
    "        log_sigma = self._log_sigma_model(feature.squeeze())\n",
    "        return mean, log_sigma\n",
    "        ...\n",
    "\n",
    "    def decoder(self, z: torch.Tensor):\n",
    "        '''\n",
    "        Implements p_{\\phi}(x | z)\n",
    "        '''\n",
    "        return self._decoder(z)\n",
    "\n",
    "    def elbo(self, data: torch.Tensor):\n",
    "        \"\"\"\n",
    "         \\mathcal{L}_{\\theta, \\phi} = E_{z\\sim q_{\\theta}(z)}[\\log(P(\\mathcal{D}|z)P(z)) - \\log q_{\\theta}(z)]\n",
    "         Assume that z = g_\\theta(x, t), where t \\sim N(0, I), then\n",
    "         = E_{q_{\\theta}}[\\log P(D|g(t, \\epsilon)) P(g(t, \\epsilon)) - \\log q_{\\theta}(g(t, \\epsilon))] \n",
    "        \"\"\"\n",
    "        n_b, n_c, h, w = data.shape\n",
    "        # p(z|x) is approximated by the encoder. p(x|z) is approximated by the decoder.\n",
    "        t = self._prior_dist.sample((n_b,))\n",
    "        dev_t = t.to(data.device)\n",
    "        mean, log_sigma = self.encoder(data)\n",
    "        dev_z = mean + torch.exp(log_sigma) * dev_t\n",
    "        dev_z = dev_z.unsqueeze(-1).unsqueeze(-1)\n",
    "        output = self.decoder(dev_z)\n",
    "\n",
    "        # The decoder parametrizes a gaussian with a mean given by the output and an identity covariance\n",
    "        # This corresponds to an MSE loss\n",
    "        log_p_x_given_z = -0.5 * (data - output) ** 2\n",
    "        log_p_x_given_z = torch.sum(log_p_x_given_z, (-1, -2, -3))\n",
    "\n",
    "        # We prescribe that the latent distribution is a guassian with zero mean and unit variance.\n",
    "        # This term encourages the encoder to keep mean and log sigma small\n",
    "        log_p_z = -0.5 * dev_z ** 2 \n",
    "        log_p_z = torch.sum(log_p_z, -1)\n",
    "\n",
    "        # Since we reparametrized sampling from sampling from z ~ q(z|x) to sampling from t~N(0, I), the Cdf of q is\n",
    "        # related to the CDF of N(0, I) through the change of variables formula Q(z) = F(g(t)). Then the pdf is the \n",
    "        # derivative with respect to t, q(z) = f(t)|det(J)| where J is the jacobian dz/dt. Since z = mu + exp(log sigma) * t,\n",
    "        # the jacobian is simply exp(log sigma).\n",
    "        z = dev_z.to('cpu')\n",
    "        log_q_z_given_z = self._prior_dist.log_prob(t.squeeze()).to(data.device) + torch.abs(torch.sum(log_sigma, -1))\n",
    "\n",
    "        return torch.mean(log_p_x_given_z + log_p_z - log_q_z_given_z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e616863-6ac3-4d82-8cad-8a5979356dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VariationalAutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b6c749-3374-498e-920f-aa9c6147936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(0.5, 0.5)\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.MNIST('/home/erick/scratch', download=True, transform=transforms)\n",
    "\n",
    "subset = torch.utils.data.Subset(dataset, [0]*256*100)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(subset, batch_size=256, shuffle=True)\n",
    "\n",
    "\n",
    "model = model.cuda()\n",
    "model = model.train()\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "image = dataset[0][0].unsqueeze(0).cuda()\n",
    "\n",
    "for epoch_idx in range(10):\n",
    "    for i, (batch, labels) in enumerate(loader):\n",
    "        optim.zero_grad()\n",
    "        batch = batch.cuda()\n",
    "        loss = -model.elbo(batch)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "        if i % 100 == 0:\n",
    "            print(epoch_idx, i, loss)\n",
    "    mean, log_sigma = model.encoder(image)\n",
    "    print(torch.norm(mean, p=1), torch.mean(torch.exp(log_sigma)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbc76e-837e-4e96-9515-8875c3b028fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(dataset[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2c691c-fc2f-4b66-884e-5b90ca0d372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[0][0].unsqueeze(0).cuda()\n",
    "\n",
    "mean, log_sigma = model.encoder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde0339-e826-45da-a915-7f251b9e2787",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.norm(mean)\n",
    "torch.exp(log_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0a8a99-085e-4efe-a54b-854bb393b3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
