{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0a0d6a-5cb9-4313-9a23-86aeb9bd025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "import torch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use('ggplot')\n",
    "\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a3c4f-1f36-45f4-bdaf-ea49cf50f0f8",
   "metadata": {},
   "source": [
    "\n",
    "Given the following distribution:\n",
    "\n",
    "$P(z) = Exp(\\lambda=1) = \\lambda \\exp(-\\lambda z)$\n",
    "\n",
    "$P(x|z) = \\textrm{Normal}(\\mu=z, \\sigma^2=1)$\n",
    "\n",
    "We want to model $P(z|x)$, which is commonly known as the posterior distribution.\n",
    "\n",
    "Typically, this quantity could be computed using Bayes rule.\n",
    "\n",
    "$P(z|x) = \\frac{P(x|z)P(z)}{P(x)} = \\frac{P(x|z)P(z)}{\\int P(x|z)P(z) dz}$\n",
    "\n",
    "We note that the integral in the denominator is intractable, so instead, we choose a surrogate distribution $q_{\\theta}(z)$ and we try to get it as close as possible to $P(z|x)$. We attempt to find the $\\theta$ such that the the probability of the observed $x$ is maximized. That is:\n",
    "\n",
    "$$\\arg\\min_{q_{\\theta}} -E_{z \\sim q_{\\theta}(z)}[\\log P(x)]$$\n",
    "\n",
    "We note that:\n",
    "\\begin{align}\n",
    "\\log P(x)  = E_{z \\sim q_{\\theta}(z)}[\\log P(x)] &= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(P(x) \\frac{q_{\\theta}(z)}{q_{\\theta}(z)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{P(z|x)} \\frac{q_{\\theta}(z)}{q_{\\theta}(z)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{q_{\\theta}(z)} \\frac{q_{\\theta}(z)}{P(z|x)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{q_{\\theta}(z)}\\right) + \\log \\left( \\frac{q_{\\theta}(z)}{P(z|x)}\\right)\\right] \\\\\n",
    "&= E_{z \\sim q_{\\theta}(z)}\\left[\\log \\left(\\frac{P(x, z)}{q_{\\theta}(z)}\\right)\\right] + E_{z \\sim q_{\\theta}(z)}\\left[ \\log \\left( \\frac{q_{\\theta}(z)}{P(z|x)}\\right)\\right] \\\\\n",
    "&= \\mathcal{L_{\\theta}} + D_{KL}(q_{\\theta}(z) || P(z|x)) \\\\\n",
    "\\log P(x) &\\geq \\mathcal{L_{\\theta}}\n",
    "\\end{align}\n",
    "\n",
    "Since, the KL divergence is non-negative, $\\mathcal{L_{\\theta}}$ is known as the evidence lower bound, or the ELBO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcc766c-0997-47c2-bfd7-84d86cd4ec59",
   "metadata": {},
   "source": [
    "Assume that we have observed a dataset $\\mathcal{D} = \\{x_i\\}_{i=1}^{N}$. Then, $P(\\mathcal{D}|z) = \\prod_{i=1}^{N} P(x_i|z)$.\n",
    "\n",
    "The ELBO is then:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\theta} &= E_{z\\sim q_{\\theta}(z)}[\\log(P(\\mathcal{D}, z)) - \\log(q_{\\theta}(z))] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}[\\log(P(\\mathcal{D}|z)P(z)) - \\log q_{\\theta}(z)] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}[\\log P(\\mathcal{D}|z) + \\log P(z) - \\log q_{\\theta}(z)] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}\\left[\\sum_{i=1}^{N}\\log P(x_i|z) + \\log P(z) - \\log q_{\\theta}(z)\\right] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}\\left[-\\frac{1}{2}\\sum_{i=1}^{N} (x_i - z)^2 - \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right] \\\\\n",
    "&= E_{z\\sim q_{\\theta}(z)}\\left[-\\frac{1}{2}\\sum_{i=1}^{N} (x_i^2 -2 x_i z + z^2) - \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right]\n",
    "\\end{align}\n",
    "\n",
    "This equation is true for all $q_{\\theta}(z)$. Now we assume that $q_{\\theta}(z) = Exp(\\theta)$.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\theta} &= E_{z\\sim q_{\\theta}(z)}\\left[-\\frac{1}{2}\\sum_{i=1}^{N} (x_i^2 -2 x_i z + z^2) - \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right] \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N} E_{z\\sim q_{\\theta}(z)}\\left[x_i^2 -2 x_i z + z^2\\right] + E_{z\\sim q_{\\theta}(z)}\\left[- \\frac{N}{2} \\log(2\\pi) + \\log \\lambda - \\lambda z - \\log \\theta + \\theta z \\right] \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N} \\left(x_i^2 - \\frac{2}{\\theta}x_i + \\frac{2}{\\theta^2}\\right) - \\frac{N}{2} \\log(2\\pi) + \\log\\lambda - \\frac{\\lambda}{\\theta} - \\log\\theta + 1 \\\\\n",
    "&= -\\frac{1}{2}\\sum_{i=1}^{N} x_i^2 + \\frac{1}{\\theta}\\sum_{i=1}^N x_i - \\frac{N}{\\theta^2} - \\frac{N}{2} \\log(2\\pi) + \\log\\lambda - \\frac{\\lambda}{\\theta} - \\log\\theta + 1 \\\\\n",
    "&= \\frac{1}{\\theta} \\left(\\sum_{i=1}^{N} (x_i) - \\lambda \\right) - \\frac{N}{\\theta^2} - \\log \\theta + C\n",
    "\\end{align}\n",
    "\n",
    "To compute the optimal $\\theta^*$, we need to compute the $\\theta$ for which $\\nabla_{\\theta} \\mathcal{L}_{\\theta} = 0$. Let $B = \\sum_{i=1}^{N} (x_i) - \\lambda$, then:\n",
    "\n",
    "\\begin{align}\n",
    "0 &= \\nabla_{\\theta} \\mathcal{L}_{\\theta} \\\\\n",
    "&= \\nabla_{\\theta} \\left(\\frac{1}{\\theta} B - \\frac{N}{\\theta^2} - \\log \\theta + C\\right) \\\\\n",
    "&= \\frac{-1}{\\theta^2} B + \\frac {2N}{\\theta^3} - \\frac{1}{\\theta} \\\\\n",
    "&= - \\theta^2 -\\theta B + 2N \n",
    "\\end{align}\n",
    "\n",
    "This is a quadratic equation in $\\theta$ with solutions:\n",
    "$$\\theta^* = \\frac{-B}{2} \\pm \\frac{1}{2}\\sqrt{B^2 + 8 N}$$\n",
    "\n",
    "Note that $\\theta$ is constrained to be positive, so one of the solutions may not be feasible.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb228e-d257-44cf-8355-274bb0f295fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_lambda = 1.0\n",
    "prior_dist = torch.distributions.Exponential(prior_lambda)\n",
    "torch.manual_seed(0)\n",
    "sampled_z = prior_dist.sample()\n",
    "\n",
    "normal_std = 1.0\n",
    "observation_dist = torch.distributions.Normal(sampled_z, normal_std)\n",
    "sampled_x = observation_dist.sample((5,))\n",
    "# sampled_x = torch.tensor(2.3)\n",
    "\n",
    "print(f'z: {sampled_z} x: {sampled_x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a7b92-b3e1-4e02-b01c-8f41a33cb3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Zs = torch.linspace(0, 15, 2000, dtype=torch.float64)\n",
    "joint_evals = []\n",
    "for z in Zs:\n",
    "    obs_dist = torch.distributions.Normal(z, normal_std)\n",
    "    log_obs_prob = torch.sum(obs_dist.log_prob(sampled_x))\n",
    "    joint_evals.append(prior_dist.log_prob(z) + log_obs_prob)\n",
    "joint_evals = torch.exp(torch.stack(joint_evals))\n",
    "approx_posterior = joint_evals / torch.sum(joint_evals * Zs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058305f5-8761-4ecd-be76-83bc97a39456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming an exponential posterior, find the optimal theta\n",
    "B = torch.sum(sampled_x) - prior_lambda\n",
    "theta_star = 0.5 * (-B + torch.sqrt(B**2 + 8 * sampled_x.numel()))\n",
    "display(Math(rf'$\\theta^*={theta_star:3f}$'))\n",
    "variational_posterior = torch.distributions.Exponential(theta_star)\n",
    "variational_evals = torch.exp(variational_posterior.log_prob(Zs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f428021f-b6ea-4261-870b-e439726952af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Zs, joint_evals, label='joint')\n",
    "plt.plot(Zs, approx_posterior, label='numerical posterior')\n",
    "plt.plot(Zs, variational_evals, label='variational posterior')\n",
    "plt.xlabel('Z')\n",
    "plt.ylabel('$p(Z|X_{obs})$')\n",
    "plt.xlim(0, 5)\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f8201f-23fa-4087-9966-9dafb35a7f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KL(Zs, numerical_posterior, variational_posterior):\n",
    "    delta = Zs[1]\n",
    "    variational_vals = variational_posterior.log_prob(Zs)\n",
    "    exp_variational_vals = torch.exp(variational_vals)\n",
    "    numerical_vals = torch.log(numerical_posterior)\n",
    "    return delta * torch.sum(exp_variational_vals * (variational_vals - numerical_vals))\n",
    "    \n",
    "thetas = torch.linspace(0.5, 2, 500, dtype=torch.float64)\n",
    "kl_divergences = []\n",
    "for theta in thetas:\n",
    "    kl_divergences.append(compute_KL(Zs, approx_posterior, torch.distributions.Exponential(theta)))\n",
    "    # break\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(thetas, kl_divergences)\n",
    "plt.ylabel('KL Divergence')\n",
    "plt.xlabel(r'$\\theta$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c2e97-3812-4153-b271-09c765237384",
   "metadata": {},
   "source": [
    "Note that the plot doesn't show a minimum at the expected location because the KL divergence is only computed for a subset of the range $[0, \\infty)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faccfdb2-6557-40f4-b191-56f0be5d6990",
   "metadata": {},
   "source": [
    "Now we try to optimize the surrogate using pytorch.\n",
    "\n",
    "We want to optimize the ELBO:\n",
    "\\begin{align}\n",
    "\\mathcal{L}_{\\theta} &= E_{z\\sim q_{\\theta}(z)} \\left[\\log \\frac{p(\\mathcal{D},z)}{q_{\\theta}(z)} \\right]\n",
    "\\end{align}\n",
    "\n",
    "Which means that we need to compute $\\nabla_{\\theta}\\mathcal{L}_{\\theta}$. This is difficult because the expectation is with respect to the distribution which is a function of $\\theta$. We then apply the reparameterization trick. Let $q_{\\theta}(z) = g_{\\theta}(x, t)$, where $t\\sim\\epsilon$ is a fixed noise distribution. Then $E_{z\\sim q_{\\theta}}[f(z)] = E_{t \\sim \\epsilon}[f(g_{\\theta}(x, t))]$. This expectation can be estimated by drawing samples from $\\epsilon$ and computing the sample mean of $f(g_{\\theta}(x, t))$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c27ebf-b5d0-4cfa-9207-f2a5a434dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we try to train find the parameter through optimization\n",
    "\n",
    "class VariationalModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VariationalModel, self).__init__()\n",
    "\n",
    "        self.param = torch.nn.Parameter(data=torch.tensor([1.0]))\n",
    "        self.sampling_dist = torch.distributions.Exponential(1.0)\n",
    "        self.prior = torch.distributions.Exponential(1.0)\n",
    "\n",
    "    def elbo(self, data):\n",
    "        sampled_z = self.sampling_dist.sample()\n",
    "        # Reparameterization trick for exponential distributions\n",
    "        reparametrized_z = sampled_z / self.param\n",
    "\n",
    "        # compute the probability of the data\n",
    "        # p(D, z) = p(z) * prod(p(x_i, z) for x_i in data)\n",
    "        log_p_z = self.prior.log_prob(reparametrized_z)\n",
    "        obs = torch.distributions.Normal(reparametrized_z, 1.0)\n",
    "        log_data_given_z = torch.sum(obs.log_prob(data))\n",
    "\n",
    "        log_model = torch.distributions.Exponential(self.param).log_prob(reparametrized_z)\n",
    "        return log_p_z + log_data_given_z - log_model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe509060-51d3-46ec-8ee9-39346ed7e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VariationalModel()\n",
    "model.train()\n",
    "opt = torch.optim.SGD(model.parameters(), lr=1e-5)\n",
    "\n",
    "for i in range(40000):\n",
    "    opt.zero_grad()\n",
    "\n",
    "    loss = -model.elbo(sampled_x)\n",
    "    loss.backward()\n",
    "    if i % 1000 == 0:\n",
    "        print(i, loss.detach(), model.param.data)\n",
    "    opt.step()    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67cd66d-73ed-4d21-ba00-be928ffaf874",
   "metadata": {},
   "source": [
    "This is really close to the optimal value we found above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0920c70-4294-42de-b9df-d9ff491d33d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
