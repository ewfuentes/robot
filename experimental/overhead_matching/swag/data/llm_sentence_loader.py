"""Loader for LLM-generated sentences from JSONL files.

Loads sentences generated by LLMs from OSM tags and indexes them by
pruned landmark properties for matching with template-based sentences.
"""

import json
import random
from pathlib import Path

from experimental.overhead_matching.swag.model.semantic_landmark_utils import (
    custom_id_from_props,
    load_all_jsonl_from_folder,
    make_sentence_dict_from_json,
    prune_landmark,
)


def load_llm_sentences_from_jsonl(
    sentences_jsonl_path: Path,
) -> dict[str, str]:
    """Load LLM sentences from a JSONL file.

    The JSONL file should contain OpenAI batch API responses with format:
    {"custom_id": "...", "response": {"body": {"choices": [{"message": {"content": "..."}}]}}}

    Args:
        sentences_jsonl_path: Path to deduplicated_sentences.jsonl file

    Returns:
        Dictionary mapping custom_id (hash) to sentence string
    """
    sentence_jsons = []
    with open(sentences_jsonl_path) as f:
        for line in f:
            sentence_jsons.append(json.loads(line))

    sentences_by_custom_id, _ = make_sentence_dict_from_json(sentence_jsons)
    return sentences_by_custom_id


def load_llm_sentences_from_directory(
    sentences_dir: Path,
) -> dict[str, str]:
    """Load LLM sentences from all JSONL files in a directory.

    Args:
        sentences_dir: Path to directory containing sentence JSONL files

    Returns:
        Dictionary mapping custom_id (hash) to sentence string
    """
    sentence_jsons = load_all_jsonl_from_folder(sentences_dir)
    sentences_by_custom_id, _ = make_sentence_dict_from_json(sentence_jsons)
    return sentences_by_custom_id


def build_pruned_to_custom_id_map(
    landmarks_tags: list[dict[str, str]],
) -> dict[frozenset, str]:
    """Build mapping from pruned_tags to custom_id.

    Args:
        landmarks_tags: List of tag dictionaries for each landmark

    Returns:
        Dictionary mapping pruned_tags (frozenset) to custom_id (hash string)
    """
    pruned_to_custom_id = {}
    for tags in landmarks_tags:
        pruned = prune_landmark(tags)
        custom_id = custom_id_from_props(pruned)
        pruned_to_custom_id[pruned] = custom_id
    return pruned_to_custom_id


def load_llm_sentences_by_pruned_tags(
    sentences_jsonl_path: Path,
    landmarks_tags: list[dict[str, str]],
) -> dict[frozenset, str]:
    """Load LLM sentences indexed by pruned landmark tags.

    This function:
    1. Loads LLM sentences from JSONL (indexed by custom_id hash)
    2. Builds a mapping from pruned_tags to custom_id using the landmarks
    3. Returns sentences indexed by pruned_tags for easy lookup

    Args:
        sentences_jsonl_path: Path to deduplicated_sentences.jsonl file
        landmarks_tags: List of tag dictionaries for each landmark

    Returns:
        Dictionary mapping pruned_tags (frozenset) to LLM sentence string
    """
    # Load sentences by custom_id
    sentences_by_custom_id = load_llm_sentences_from_jsonl(sentences_jsonl_path)

    # Build mapping from pruned_tags to sentence
    out = {}
    for tags in landmarks_tags:
        pruned = prune_landmark(tags)
        custom_id = custom_id_from_props(pruned)
        if custom_id in sentences_by_custom_id:
            out[pruned] = sentences_by_custom_id[custom_id]

    return out


def load_llm_sentences_by_pruned_tags_from_dir(
    sentences_dir: Path,
    landmarks_tags: list[dict[str, str]],
) -> dict[frozenset, str]:
    """Load LLM sentences from a directory, indexed by pruned landmark tags.

    Args:
        sentences_dir: Path to directory containing sentence JSONL files
        landmarks_tags: List of tag dictionaries for each landmark

    Returns:
        Dictionary mapping pruned_tags (frozenset) to LLM sentence string
    """
    # Load sentences by custom_id
    sentences_by_custom_id = load_llm_sentences_from_directory(sentences_dir)

    # Build mapping from pruned_tags to sentence
    out = {}
    for tags in landmarks_tags:
        pruned = prune_landmark(tags)
        custom_id = custom_id_from_props(pruned)
        if custom_id in sentences_by_custom_id:
            out[pruned] = sentences_by_custom_id[custom_id]

    return out


def get_coverage_stats(
    llm_sentences: dict[frozenset, str],
    total_landmarks: int,
) -> dict:
    """Get statistics about LLM sentence coverage.

    Args:
        llm_sentences: Dictionary mapping pruned_tags to sentences
        total_landmarks: Total number of landmarks

    Returns:
        Dictionary with coverage statistics
    """
    return {
        "llm_sentences_count": len(llm_sentences),
        "total_landmarks": total_landmarks,
        "coverage_percent": len(llm_sentences) / total_landmarks * 100 if total_landmarks > 0 else 0,
    }


def split_llm_sentences(
    llm_sentences: dict[frozenset, str],
    train_fraction: float = 0.9,
    seed: int = 42,
) -> tuple[dict[frozenset, str], dict[frozenset, str]]:
    """Split LLM sentences into train/test sets by pruned_tags.

    Args:
        llm_sentences: Dictionary mapping pruned_tags to LLM sentences
        train_fraction: Fraction of items for training
        seed: Random seed for reproducibility

    Returns:
        Tuple of (train_llm_sentences, test_llm_sentences)
    """
    keys = list(llm_sentences.keys())
    rng = random.Random(seed)
    rng.shuffle(keys)

    split_idx = int(len(keys) * train_fraction)
    train_keys = set(keys[:split_idx])

    train = {k: v for k, v in llm_sentences.items() if k in train_keys}
    test = {k: v for k, v in llm_sentences.items() if k not in train_keys}
    return train, test
